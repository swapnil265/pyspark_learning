{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3b60d4-abc3-4a3e-8cd3-dbbc064c3800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"/Users/rajswapnil265@gmail.com/Quickstart Notebook\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6f4e9d-460e-4285-95e4-b05fbebdf455",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+------+\n|CustomerID| OrderDate|Product|Quantity| Price|\n+----------+----------+-------+--------+------+\n|      C001|2024-03-22|   Book|       3| 44.99|\n|      C002|2024-03-22| Laptop|       1|799.99|\n|      C003|2024-03-23|    Pen|      10|  1.99|\n|      C004|2024-03-23|Monitor|       2|149.99|\n|      C005|2024-03-24|   Desk|       1|199.99|\n+----------+----------+-------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CustomerOrders\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"C001\", \"2024-03-22\", \"Book\", 3, 44.99),\n",
    "    (\"C002\", \"2024-03-22\", \"Laptop\", 1, 799.99),\n",
    "    (\"C003\", \"2024-03-23\", \"Pen\", 10, 1.99),\n",
    "    (\"C004\", \"2024-03-23\", \"Monitor\", 2, 149.99),\n",
    "    (\"C005\", \"2024-03-24\", \"Desk\", 1, 199.99)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93caede5-04ef-498a-afe5-3ff52427a1ad",
     "showTitle": true,
     "title": "Lambda Function conversion"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 8, 10]\n[4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "my_list=[2,3,4,5]\n",
    "def doubler(x):\n",
    "    return 2*x\n",
    "\n",
    "output=list(map(doubler,my_list))\n",
    "print(output)\n",
    "#Map maps each input in the 2nd to the first and returns its running on gateway node \n",
    "#pyspark map basically provides the parallelism and runs on the multiple in the node \n",
    "#  Lets see how we can make a lambda here embedding the same in the lambda we want to use it one time \n",
    "\n",
    "output_lambda = list(map(lambda x:2*x , my_list))\n",
    "print(output_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a2aa34-c307-4b25-8814-516c41535e47",
     "showTitle": true,
     "title": "Q1.Write a function that adds up all the element in list"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n10\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce \n",
    "\n",
    "my_list=[1,2,3,4]\n",
    "def add(x):\n",
    "    total=0\n",
    "    for i in x:\n",
    "        total=total+ i\n",
    "    return total\n",
    "output=add(my_list)\n",
    "print(output)\n",
    "\n",
    "#COnvert to lamda we need to use reduce method\n",
    "#when one to one mapping required we use map \n",
    "#when many to one reduce \n",
    "\n",
    "output_lamda=reduce(lambda x ,y : x+y , my_list)\n",
    "print(output_lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6590cbca-06f7-46d3-a72d-64418dfb19f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice Notebook_week4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
